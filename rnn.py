# -*- coding: utf-8 -*-
"""rnn7

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QXqjLsxJWjF1r3UpuK-UUb0U-eRwZEjS
"""

# Commented out IPython magic to ensure Python compatibility.
## Import PyDrive and associated libraries.
# This only needs to be done once in a notebook.
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
# This only needs to be done once in a notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive
import sys
sys.path.append('/gdrive/My Drive/ml/code')

import pandas as pd
import tensorflow as tf
import modelset
from collections import deque
import numpy as np
import os
os.getcwd()

#load data
data=pd.read_pickle('My Drive/ml/data/data.pkl').dropna()

class RNNtranier():
  def __init__(self,batch,epochs,trainws,valws,roll_ws,ws,name):
    #batchsize
    self._batchsize=batch

    self._epochs=epochs
    #training window size
    self._trainws=trainws

    #validation window size
    self._valws=valws

    #rolling window size  
    self._roll_ws=roll_ws

    #RNN sequence length
    self._ws=ws

    #model name
    self._name=name

  def load_data(self,data):
    self._dataset=data

  def _generate_window_train(self,start,end):
    dataset=self._dataset.query('trade_date>=@start and trade_date<@end')
    codes=dataset.stock_code.unique()
    features=[]
    labels=[]
    for code in codes:
      mask1=dataset['stock_code']==code
      dataset_code=dataset.loc[mask1,:]
      dataset_code.sort_values(by=['trade_date'],ascending=True)
      dataset_code.drop(['trade_date','stock_code','bin'],axis=1,inplace=True)
      dataset_code_ret=dataset_code.pop('ret').values
      dataset_code_features=dataset_code.values
      if len(dataset_code_ret)<self._ws:
        continue
      else:
        for i in range(self._ws-1,len(dataset_code_ret)):
          labels.append(dataset_code_ret[i-self._ws+1:i+1])
          features.append(dataset_code_features[i-self._ws+1:i+1])
    features = np.array(features, dtype=np.float32) 
    labels = np.array(labels, dtype=np.float32) 
    return features,labels

  def _generate_window_test(self,start,end):
    dataset=self._dataset.query('trade_date>=@start and trade_date<@end')
    c=dataset.stock_code.unique()
    features=[]
    labels=[]
    codes=[]
    trade_date=[]
    for code in c:
      mask1=dataset['stock_code']==code
      dataset_code=dataset.loc[mask1,:]
      dataset_code.sort_values(by=['trade_date'],ascending=True)
      dataset_date=dataset_code.pop('trade_date').values
      dataset_code.drop(['stock_code','bin'],axis=1,inplace=True)
      dataset_code_ret=dataset_code.pop('ret').values
      dataset_code_features=dataset_code.values
      if len(dataset_code_ret)<self._ws:
        continue
      else:
        for i in range(self._ws-1,len(dataset_code_ret)):
          labels.append(dataset_code_ret[i])
          features.append(dataset_code_features[i-self._ws+1:i+1])
          codes.append(str(code))
          trade_date.append(dataset_date[i].astype('datetime64[s]').tolist().strftime("%Y%m%d"))
    codes = np.array(codes, dtype=np.float32) 
    trade_date = np.array(trade_date, dtype=np.float32)    
    features = np.array(features, dtype=np.float32) 
    return features,codes,trade_date,labels

  def _get_train_data(self,start,end):
    features,labels=self._generate_window_train(start,end)
    train_dataset = tf.data.Dataset.from_tensor_slices((features, labels)).shuffle(10000).batch(self._batchsize).prefetch(tf.data.experimental.AUTOTUNE)
    return train_dataset

  def _get_val_data(self,start,end):
    features,labels=self._generate_window_train(start,end)
    return features,labels
    
  @tf.function
  def _train_step(self,x_batch_train, y_batch_train):
    with tf.GradientTape() as tape:
      y_batch_predicted = model(x_batch_train) 
      loss_value =loss_fn(y_batch_train, y_batch_predicted)
    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    train_acc_metric.update_state(y_batch_train, y_batch_predicted)
  
  def _validate_step(self,x_batch_val,y_batch_val):
    val_acc_metric.update_state(y_batch_val,model(x_batch_val))

  @property
  def _datelist(self):
    datelist=self._dataset.trade_date.unique()
    datelist=np.sort(datelist)
    return datelist

  def roll_train(self):
    datelist=self._datelist
    ckpt.restore(manager.latest_checkpoint)
    if manager.latest_checkpoint:
      print("Restored from {}".format(manager.latest_checkpoint))
    else:
      print("Initializing from scratch.")
    train_start=0+ckpt.step.numpy()*self._roll_ws
    train_end=train_start+self._trainws
    val_start=train_end+5
    val_end=val_start+self._valws
    test_start=val_end+5
    test_end=test_start+self._roll_ws

    while test_end<len(datelist):
      val_queue=deque()
      train_dataset=self._get_train_data(datelist[train_start],datelist[train_end])
      x_batch_val, y_batch_val=self._get_val_data(datelist[val_start],datelist[val_end])
      summary_writer = tf.summary.create_file_writer(
      'My Drive/ml/{}/training_logs/'.format(self._name)+ datelist[train_start].astype('datetime64[s]').tolist().strftime("%Y%m%d")+'_'+datelist[train_end].astype('datetime64[s]').tolist().strftime("%Y%m%d")+'_'+datelist[val_start].astype('datetime64[s]').tolist().strftime("%Y%m%d")+'_'+datelist[val_end].astype('datetime64[s]').tolist().strftime("%Y%m%d"))
      with summary_writer.as_default():
        for epoch in range(self._epochs):
          for x_batch_train, y_batch_train in train_dataset:
            self._train_step(x_batch_train, y_batch_train)
          self._validate_step(x_batch_val,y_batch_val)
          val_queue.append(val_acc_metric.result())
          tf.summary.scalar('tran_mse', train_acc_metric.result(), step=epoch)
          tf.summary.scalar('val_mse', val_acc_metric.result(), step=epoch)
          val_acc_metric.reset_states()
          train_acc_metric.reset_states()

          #early stopping
          if len(val_queue)>=41:
            potential_saddle=val_queue.pop()
            if min(val_queue)>potential_saddle:
                break
      del train_dataset
      checkpoint_dir = 'My Drive/ml/{}/training_weights'.format(self._name)
      checkpoint_prefix = os.path.join(checkpoint_dir, 
                                       datelist[train_start].astype('datetime64[s]').tolist().strftime("%Y%m%d")+'_'+datelist[train_end].astype('datetime64[s]').tolist().strftime("%Y%m%d"))
      model.save_weights(checkpoint_prefix)
      ckpt.step.assign_add(1)
      manager.save()
      print(datelist[train_start].astype('datetime64[s]').tolist().strftime("%Y%m%d"))
      train_start=self._roll_ws+train_start
      train_end=train_start+self._trainws
      val_start=train_end+5
      val_end=val_start+self._valws
      test_start=val_end+5
      test_end=test_start+self._roll_ws

  def roll_predict(self):
    datelist=self._datelist
    train_start=0
    train_end=train_start+self._trainws
    val_start=train_end+5
    val_end=val_start+self._valws
    test_start=val_end+5
    test_end=test_start+self._roll_ws
    trade_date_list=[]
    stock_code_list=[]
    y_hat_list=[]
    while test_end<len(datelist):
      loc='My Drive/ml/{}/training_weights/'.format('rnn7')+datelist[train_start].astype('datetime64[s]').tolist().strftime("%Y%m%d")+'_'+datelist[train_end].astype('datetime64[s]').tolist().strftime("%Y%m%d")
      model.load_weights(loc)
      loc='My Drive/ml/{}/training_weights/'.format('rnn8')+datelist[train_start].astype('datetime64[s]').tolist().strftime("%Y%m%d")+'_'+datelist[train_end].astype('datetime64[s]').tolist().strftime("%Y%m%d")
      model1.load_weights(loc)
      features,codes,dates,_=self._generate_window_test(datelist[test_start-self._ws+1],datelist[test_end])
      if len(features)==0:
        continue
      predict_values=(model(features,training=False)[:,-1,0]+model1(features,training=False)[:,-1,0])/2
      trade_date_list.append(dates)
      stock_code_list.append(codes)
      y_hat_list.append(predict_values.numpy())
      train_start=self._roll_ws+train_start
      train_end=train_start+self._trainws
      val_start=train_end+5
      val_end=val_start+self._valws
      test_start=val_end+5
      test_end=test_start+self._roll_ws
      print(test_end)
    y_hat_list=np.concatenate(np.array(y_hat_list).reshape(-1,))
    trade_date_list=np.concatenate(np.array(trade_date_list).reshape(-1,))
    stock_code_list=np.concatenate(np.array(stock_code_list).reshape(-1,))
    results=pd.DataFrame(data={"trade_date":trade_date_list,"stock_code":stock_code_list,"y_hat":y_hat_list})
    results.to_csv('My Drive/ml/rnn7.csv',index=False)
    return trade_date_list

train_acc_metric = tf.keras.metrics.MeanSquaredError()
val_acc_metric = tf.keras.metrics.MeanSquaredError()
loss_fn=tf.keras.losses.MeanSquaredError()
optimizer=tf.keras.optimizers.Adam(learning_rate=0.0033)
model=modelset.rnn3()
model1=modelset.rnn3()
ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=optimizer, model=model)
manager = tf.train.CheckpointManager(ckpt, 'My Drive/ml/rnn8/training_checkpoints', max_to_keep=3)
trainer=RNNtranier(512,200,250,20,20,3,'rnn8')
trainer.load_data(data)

trainer.roll_train()

x=trainer.roll_predict()

